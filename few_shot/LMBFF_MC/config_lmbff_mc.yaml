task: classification
num_classes: 5
k: 16

data:
  dataset_file_path: Data/HiQualProp_with_author_feats/df_tweets_HiQualProp_with_author_feats.csv
  dataset_save_path: few_shot/data_splitted
  need_data_prep: False
  label: strategy
  text: text_normalized
  sample_hierarchy: # ordering matters! only allows following three combos: (labels, strategy), (labels), (strategy)
    - labels
    - strategy
  meta_feats: False
  meta_feat_names_path: Data/author_feat_names.txt

logging:
  path: few_shot/LMBFF_MC/

checkpoint:
  higher_better: True

seeds: #length of this list corresponds to the number of runs (& different data splits and fewshot samples)
  - 123
  - 234
  - 345
  - 456
  - 567

auto_t: True
auto_v: True
template_file_path: few_shot/LMBFF_MC/template_for_auto_t_mtke.txt
verbalizer_file_path: few_shot/LMBFF_MC/manual_v_mtke.txt

plm:
  model_name: roberta
  model_path: roberta-large
  specials_to_add:
  num_freeze_layers: 16
  optimize:
    freeze_para: False
    lr: 0.00003
    betas: [0.9, 0.999]
    eps: 0.000001
    weight_decay: 0.01
    no_decay:
      - bias
      - LayerNorm.weight
    scheduler:
      type: 
      num_warmup_steps: 50

template_generator:
  plm:
    model_name: t5
    model_path: t5-large
    specials_to_add:
  beam_width: 50
  decoder_max_length: 128
  max_seq_length: 128
  batch_size: 8

verbalizer_generator:
  candidate_num: 50
  label_word_num_per_class: 50
  batch_size: 8

dataset:
  label_path_sep: '-'

train:
  num_epochs: 50
  gradient_accumulation_steps: 1
  batch_size: 4
  clean: True
  max_grad_norm: 0

environment:
  cuda: True
  num_gpus: 2
  cuda_visible_devices:
    - 0
    - 1
  local_rank: 0
  model_parallel:

template: mixed_template
verbalizer: manual_verbalizer

mixed_template:
  choice: 0
  file_path: few_shot/LMBFF_MC/template_for_auto_t_mtke.txt

manual_verbalizer:
  choice: 0
  file_path: few_shot/LMBFF_MC/manual_v_mtke.txt

classification:
  metric: 
    - accuracy
    - micro-f1
    #- precision
    #- recall
  test_metric:
    - accuracy
    - weighted-f1
    - macro-f1
    - weighted-precision
    - weighted-recall
    #- weighted-auc
  loss_function: cross_entropy
  auto_t: True
  auto_v: False